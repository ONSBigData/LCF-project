{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucene Scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Lucene Similarity: a modified TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In information retrieval, [**tf–idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for **term frequency–inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "Lucene's Practical Scoring Function uses a modified version of the TF-IDF found in textbooks, whihc is explained below.\n",
    "\n",
    "### Lucene modified TF\n",
    "\n",
    "Instead of taking the TF as is, Lucene uses $\\sqrt{TF}$. This way, documents with twice the number of terms as another document aren’t twice as relevant. Instead the TF score is computed as follows:\n",
    "\n",
    "| TF | Lucene TF |\n",
    "|------|------|\n",
    "|   1  | 1 |\n",
    "|   2  | 1.141 |\n",
    "|   4  | 2 |\n",
    "|   8  | 2.828 |\n",
    "|   16  | 4 |\n",
    "\n",
    "\n",
    "### Lucene modified IDF\n",
    "\n",
    "Similarly users don’t consider terms that only occur in 10 documents ten times as special as those that occur in 100 documents. Instead, the IDF score is computed as:\n",
    "\n",
    "$$ln\\left ( \\frac{docCount}{docFreq_{t} + 1} \\right ) + 1$$\n",
    "\n",
    "where *docCount* is the total number of documents in the collection, and *docFreq<sub>t</sub>* is the number of documents in the collection containing  term *t*.\n",
    "\n",
    "### Document length\n",
    "\n",
    "The impact of a document’s length is taken into account by multiplying the TF\\*IDF score by $\\frac{1}{\\sqrt{|d|}}$\n",
    "\n",
    "##### Lucene Lossy Compression\n",
    "\n",
    "Lucene computes the lenght of a document |d| at indexing time, i.e. every time a document is added to the index. This information is then retrieved at search time, i.e. when the query is performed. \n",
    "\n",
    "However, to efficiently store this information, Lucene performs a lossy compression. The document length value is encoded as a single byte before being stored. At search time, the byte value is read from the index directory and decoded back to a float value. This encoding/decoding, while reducing index size, comes with the price of precision loss, i.e. it is not guaranteed that decode( encode(x) ) = x.\n",
    "\n",
    "The rationale supporting such lossy compression is that given the difficulty (and inaccuracy) of users to express their true information need by a query, only big differences matter.\n",
    "\n",
    "### Lucene Practical Scoring Function\n",
    "\n",
    "Lucene's Practical Scoring Function (simplified version) is as follows: \n",
    "\n",
    "$$score_{q,\\; d}   =   coord_{q, d} \\;  \\times \\;  queryNorm_{q} \\;  \\times \\; \\sum_{t\\;  in\\; q} \\left ( tf_{t, d}\\;  \\times \\; idf_{t}^{2} \\;  \\times \\;  norm_{d}  \\right )$$\n",
    "\n",
    "where:\n",
    "\n",
    "* **coord<sub>q, d</sub>** : is a score factor based on how many of the query terms are found in the specified document. For example, if documnt *d* contains 2 of the 3 terms of query *q*, then coord<sub>q, d</sub> is 2/3.\n",
    "* **queryNorm<sub>q</sub>** : is a normalizing factor used to make scores between queries comparable. This factor does not affect document ranking (since all ranked documents are multiplied by the same factor), but rather just attempts to make scores from different queries (or even different indexes) comparable. The default computation produces a Euclidean norm:\n",
    "\n",
    "$$ queryNorm_{q} =\\frac{1}{\\sqrt{sumOfSquaredWeights}}   $$\n",
    "\n",
    "    where\n",
    "\n",
    "$$ sumOfSquaredWeights = \\sum_{t \\; in \\; q} idf_{t}^{2} $$\n",
    "\n",
    "* **tf<sub>t, d</sub>**: correlates to the term's frequency, defined as the number of times term t appears in the currently scored document d. Documents that have more occurrences of a given term receive a higher score. It is computed as explained above.\n",
    "\n",
    "* **idf<sub>t</sub>** : stands for Inverse Document Frequency. This value correlates to the inverse of docFreq (the number of documents in which the term t appears). This means rarer terms give higher contribution to the total score. idf<sub>t</sub> appears for t in both the query and the document, hence it is squared in the equation. It is computed as explained above.\n",
    "\n",
    "* **norm<sub>d</sub>** : encapsulates a (indexing time) length factor computed and stored as an 8 bit floating point value which causes some loss of precison (lossy compression).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement encoding and decoding function, as done here \n",
    "# https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/util/SmallFloat.java\n",
    "def floatToRawIntBits(f):\n",
    "    s = struct.pack('=f', f)\n",
    "    return struct.unpack('=l', s)[0]\n",
    "\n",
    "def intBitsToFloat(b):\n",
    "    s = struct.pack('>l', b)\n",
    "    return struct.unpack('>f', s)[0]\n",
    "\n",
    "def byte315ToFloat(b):\n",
    "    if (b == 0):\n",
    "        return 0.0\n",
    "    bits = (b&0xff) << (24-3)\n",
    "    bits += (63-15) << 24\n",
    "    return intBitsToFloat(bits)\n",
    "\n",
    "def floatToByte315(f):\n",
    "    bits = floatToRawIntBits(f)\n",
    "    \n",
    "    smallfloat = bits >> (24-3)\n",
    "    \n",
    "    if (smallfloat <= ((63-15)<<3)):\n",
    "        return  bytes(0) if (bits<=0) else bytes(1)\n",
    "    \n",
    "    if (smallfloat >= ((63-15)<<3) + 0x100):\n",
    "        return -1\n",
    "    \n",
    "    return int(bytes(smallfloat - ((63-15)<<3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replicating the Lucene Classic Similarity Lossy Compression\n",
    "NORM_TABLE = np.arange(256, dtype= float)\n",
    "for i in range(256):\n",
    "    NORM_TABLE[i] = byte315ToFloat(int(bytes(i)))\n",
    "\n",
    "def decodeNormValue(b):\n",
    "    return NORM_TABLE[b & 0xFF] # & 0xFF maps negative bytes to positive above 127\n",
    "\n",
    "def encodeNormValue(fieldLength):\n",
    "    return floatToByte315(fieldLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Norm formula\n",
    "def norm(value):\n",
    "    length = 1.0/np.sqrt(value)\n",
    "    return float(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeNorm(docs):\n",
    "    vect = CountVectorizer(analyzer='word')\n",
    "    X = vect.fit_transform(docs)\n",
    "    docCount = X.shape[0]\n",
    "    return np.matrix(map(decodeNormValue, map(encodeNormValue, map(norm, X.sum(axis = 1))))).reshape(docCount, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeTFIDF(docs, vocab):\n",
    "    # Document-term matrix for every term in the query\n",
    "    vect = CountVectorizer(vocabulary = vocab, analyzer='word')\n",
    "    tf = vect.fit_transform(docs)\n",
    "    docCount = tf.shape[0]\n",
    "    docFreqs = (tf != 0).sum(axis = 0)\n",
    "    \n",
    "    tf = tf.sqrt()\n",
    "    \n",
    "    idf = 1.0 + np.log(np.divide(docCount, (docFreqs + 1.0)))\n",
    "    \n",
    "    idf = np.square(idf, idf)\n",
    "    \n",
    "    lengthNorm = computeNorm(docs)\n",
    "    \n",
    "    tf = tf.multiply(lengthNorm)\n",
    "\n",
    "    return (tf, idf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scores(query, docs):\n",
    "    queryTerms = query.split(' ')\n",
    "    tf, idf = computeTFIDF(docs, queryTerms)\n",
    "    queryNorm = np.divide(1.0, np.sqrt(idf.sum(axis = 0)))\n",
    "    idf = np.multiply(idf, queryNorm)\n",
    "    res = tf.dot(idf)\n",
    "    coord = np.divide((tf != 0).astype(float)*(idf != 0).astype(float), len(queryTerms))\n",
    "    res = np.multiply(res, coord)\n",
    "    return res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = pd.Series([\"Lucene Action continue continued\", \"Lucene  Dummies mbuy\", \"Managing Gigabytes\", \n",
    "                  \"Art  Computer Science\", \"Action\", \"Lucene way\", \"Managing Megabytes lucene\", \"Art Gaming\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.99041463],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 1.98082925],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"action\"\n",
    "res = scores(query, docs)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.\tAction\t1.980829\n",
      "1.\tLucene Action continue continued\t0.990415\n"
     ]
    }
   ],
   "source": [
    "candidates = res != 0\n",
    "indices = docs.index[np.asarray(candidates.T)[0,:]]\n",
    "scores = np.asarray(res[indices].T)[0,:]\n",
    "sorted_indices = indices[np.argsort(-scores)]\n",
    "for rank, idx in enumerate(sorted_indices):\n",
    "    print \"%d.\\t%s\\t%f\" %(rank, docs[idx], res[idx, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FieldBoosts and TermBoosts\n",
    "\n",
    "The above example represents a simplified version of what Lucene can do. In fact, the formula can be enriched with **FieldBoosts** and **TermBoosts**. \n",
    "\n",
    "##### FieldBoost\n",
    "Above, by document we meant a simple string of text. In reality, documents can be composed by multiple fields. For example, a book can heve the following fields: Title, Authors, Text.\n",
    "With Lucene is possible to perform a query to multiple fields, or perform different queries to different fields.\n",
    "The TDF-IDF scores are computed for each query-field pair and then summed together. Sometimes some fields might be more important then other, thus Lucene allows to specify a fieldBoost which can lower or increase the query-field score contribution to the final score. \n",
    "\n",
    "##### TermBoost\n",
    "TermBoost is a search time boost of term t in the query q as specified in the query text. For example, a query specified this way \"lucene way^0.2\" will assign a weight of 1 (the default) to the term \"lucene\" and a weight of 0.2 to the term \"way\". TermBoosts can be used to lower (&lt;1) or increase (>1) the importance of the term in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucene Query Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional things Lucene does that here are not done is the parsing of the query string and of the documents. Lucene provides several query parsers. By default, the simple parser remove stopwords and apply case fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Lucene Default Similarity: BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BM25 stands for [**Best Matching 25**](https://en.wikipedia.org/wiki/Okapi_BM25), also called *Okapi weighting scheme*, and improves upon the classical TF \\* IDF.\n",
    "\n",
    "Released in [1994](http://trec.nist.gov/pubs/trec3/t3_proceedings.html), BM25 has its roots in [probabilistic information retrieval](http://nlp.stanford.edu/IR-book/html/htmledition/probabilistic-information-retrieval-1.html). A relevance score, according to probabilistic information retrieval, reflects the probability a user will consider the result relevant. \n",
    "\n",
    "The [BM25 weighting scheme](https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java) has become the default similarity measure from **Lucene 6**.\n",
    "\n",
    "### Lucene BM25 view of IDF\n",
    "\n",
    "The [original BM25 formula for IDF](https://en.wikipedia.org/wiki/Okapi_BM25#The_ranking_function) is computed as:\n",
    "\n",
    "$$idf_{t} = \\ln (\\frac{docCount - docFreq_{t} + 0.5}{docFreq_{t} + 0.5})$$\n",
    "\n",
    "where *docCount* is the total number of documents in the collection, and *docFreq<sub>t</sub>* is the number of documents in the collection containing  term *t*.\n",
    "\n",
    "It shows potentially major drawbacks when using it for terms appearing in more than half of the corpus documents. These terms' IDF is negative, so for any two almost-identical documents, one which contains the term and one which does not contain it, the latter will possibly get a larger score. This means that terms appearing in more than half of the corpus will provide negative contributions to the final document score. This is often an undesirable behavior, so Lucene overcomes this problem by adding 1 to the value, before taking the log, which makes it impossible to compute a negative value.\n",
    "\n",
    "$$idf_{t} = \\ln (1 + \\frac{docCount - docFreq_{t} + 0.5}{docFreq_{t} + 0.5})$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Lucene BM25 view of TF and the influence of docLength\n",
    "\n",
    "Term frequency in BM25 lower the impact of term frequency even further than traditional TF \\* IDF. In the BM25, the impact of term frequency is always increasing, but asymptotically approaches the value (k + 1) (where in the default implementation k = 1.2).\n",
    "\n",
    "$$ \\frac{(k + 1) \\times tf_{t, d}}{k + tf_{t, d}} $$\n",
    "\n",
    "More *tf* always means more relevance. However in this way it quickly hits diminishing returns. Classic *tf*, on the other hand, constantly increases and never reaches a saturation point.\n",
    "\n",
    "Changing k can be a useful tuning approach to modify the impact of the *tf*. A higher k causes *tf* to take longer to reach saturation. By stretching out the point of saturation, it stretches out the relevance difference between higher and lower term frequency docs.\n",
    "\n",
    "The TF score above is further influenced by whether the document length is above or below the average length of a document in the corpus.\n",
    "The formula above is modified by adding (1.0 - b + b * L) as a multiple on k in the denominator.\n",
    "\n",
    "$$ tfNorm_{t,\\; d} =  \\frac{(k + 1) \\times  tf_{t, d}}{k \\times  (1.0 - b + b \\times  L) + tf_{t, d}} $$\n",
    "\n",
    "Here L is how long a document is relative to the average document length. L therefore is actually presented as $ \\frac{|d|}{avgDocumentLength} $, i.e. this document length divided by the average document length.\n",
    "\n",
    "The constant b (where in the default implementation b = 0.75) allows to finely tune how much influence the L value has on scoring. A b of 0 completely removes the influence of L. A higher b adds more document length influence on the scoring. \n",
    "\n",
    "### Lucene Lossy Compression\n",
    "\n",
    "Lucene computes the lenght of a document |d| at indexing time, i.e. every time a document is added to the index. This information is then retrieved at search time, i.e. when the query is performed. \n",
    "\n",
    "However, to efficiently store this information, Lucene performs a lossy compression. The document length value is encoded as a single byte before being stored. At search time, the byte value is read from the index directory and decoded back to a float value. This encoding/decoding, while reducing index size, comes with the price of precision loss, i.e. it is not guaranteed that decode( encode(x) ) = x.\n",
    "\n",
    "The rationale supporting such lossy compression is that given the difficulty (and inaccuracy) of users to express their true information need by a query, only big differences matter.\n",
    "\n",
    "\n",
    "\n",
    "### All Together\n",
    "\n",
    "Putting all together, the score of a document *d* given a query *q* (with multiple terms) is given by:\n",
    "\n",
    "$$ score_{q,\\; d} = \\sum_{t\\; in \\; q }tfNorm_{t,\\; d} \\times  idf_{t} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COICOP Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path variables\n",
    "os.chdir(\"/Users/Alessandra/Downloads/foodies\")\n",
    "ROOT = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "strip = lambda x : x.strip()\n",
    "foodies = pd.read_csv(os.path.join(ROOT, \"clean2014Q3.csv\") , sep = \",\", header=0,\n",
    "                      names=[\"coicop\", \"EXPDESC\", \"Paid1\", \"Shop\", \"MAFFQuan\", \"MAFFUnit\"],\n",
    "                      converters = {'coicop' : strip,\n",
    "                                    'EXPDESC' : strip,\n",
    "                                    'Paid1' : strip,\n",
    "                                    'Shop' : strip,\n",
    "                                    'MAFFQuan': float,\n",
    "                                    'MAFFUnit': strip})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coicop</th>\n",
       "      <th>EXPDESC</th>\n",
       "      <th>Paid1</th>\n",
       "      <th>Shop</th>\n",
       "      <th>MAFFQuan</th>\n",
       "      <th>MAFFUnit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11111</td>\n",
       "      <td>loaf wht unsl  fh</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11111</td>\n",
       "      <td>loaf wht unsl  fh</td>\n",
       "      <td>80</td>\n",
       "      <td>20</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11111</td>\n",
       "      <td>loaf wht unsl  fh</td>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "      <td>600.0</td>\n",
       "      <td>Grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11111</td>\n",
       "      <td>wht bread un sl</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11111</td>\n",
       "      <td>wht bread un sl</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Grams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  coicop            EXPDESC Paid1 Shop  MAFFQuan MAFFUnit\n",
       "0  11111  loaf wht unsl  fh    80   20     400.0    Grams\n",
       "1  11111  loaf wht unsl  fh    80   20     400.0    Grams\n",
       "2  11111  loaf wht unsl  fh    85   20     600.0    Grams\n",
       "3  11111    wht bread un sl   100   20     400.0    Grams\n",
       "4  11111    wht bread un sl   100   20     400.0    Grams"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foodies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split training and tstind data\n",
    "X_train, X_test, y_train, y_test = train_test_split(foodies[\"EXPDESC\"], \n",
    "                                                    foodies[\"coicop\"], \n",
    "                                                    test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A different lossy compression\n",
    "NORM_TABLE = np.arange(256, dtype= float)\n",
    "NORM_TABLE[0] = 0.0\n",
    "for i in range(1, 256):\n",
    "    f = byte315ToFloat(int(bytes(i)))\n",
    "    NORM_TABLE[i] = 1.0 / (f*f)\n",
    "\n",
    "NORM_TABLE[0] = 1.0 / NORM_TABLE[255]\n",
    "\n",
    "def decodeNormValue(b):\n",
    "    return NORM_TABLE[b & 0xFF]\n",
    "\n",
    "def encodeNormValue(fieldLength):\n",
    "    boost = 1.0\n",
    "    return floatToByte315(boost / float(np.sqrt(fieldLength)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute documents length\n",
    "def getFieldLength(docs, lossy):\n",
    "    \n",
    "    # Document-term matrix for every term in the documents\n",
    "    vect = CountVectorizer(analyzer='word')\n",
    "    X = vect.fit_transform(docs)\n",
    "    docCount = X.shape[0]\n",
    "    \n",
    "    # Sum all counts / Number of docs\n",
    "    avgFieldLength = X.sum()/float(docCount)\n",
    "    \n",
    "    # Encode and decode lengths for every document\n",
    "    if lossy:\n",
    "        lengths = map(encodeNormValue, X.sum(axis = 1))\n",
    "        fieldLengths = np.matrix(map(decodeNormValue, lengths)).reshape(docCount, 1)\n",
    "    else:\n",
    "        fieldLengths = np.matrix(X.sum(axis = 1)).reshape(docCount, 1)\n",
    "        \n",
    "    \n",
    "    return(fieldLengths, avgFieldLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute idf, tfNorm\n",
    "def get_tfidf(docs, vocab, lossy, k, b):\n",
    "    \n",
    "    # Document-term matrix for every term in the queries (X_test)\n",
    "    vect = CountVectorizer(vocabulary = vocab, analyzer='word')\n",
    "    tf = vect.fit_transform(docs)\n",
    "    docCount = tf.shape[0]\n",
    "    docFreqs = (tf != 0).sum(0)\n",
    "    fieldLengths, avgFieldLength = getFieldLength(docs, lossy)\n",
    "    \n",
    "    # Compute idf, tfNorm\n",
    "    idf = np.log(1 + np.divide((docCount - docFreqs + 0.5), (docFreqs + 0.5)))\n",
    "    tfNorm = np.divide((tf * (k + 1)).toarray(),(tf + k * (1 - b + b * (fieldLengths/avgFieldLength))))\n",
    "    \n",
    "    return (tfNorm, idf.T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return best neighbour class\n",
    "def best(nn):\n",
    "    mode = stats.mode(nn, axis=None, nan_policy='omit')\n",
    "    return mode.mode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(X_train, X_test, y_train, boosted = [], hintered = [], lossy = True, k = 1.2, b = 0.75):\n",
    "    \n",
    "    q = CountVectorizer(analyzer='word')\n",
    "    queries = q.fit_transform(X_test)\n",
    "    presences = (queries != 0).astype(int)\n",
    "    vocab = q.get_feature_names()\n",
    "    \n",
    "    tfNorm, idf = get_tfidf(X_train, vocab, lossy, k, b)\n",
    "    if boosted or hintered :\n",
    "        idf_2 = np.multiply(idf, np.matrix([0.5 if x in hintered else 2 if x in boosted else 1 for x in vocab]).T)\n",
    "    \n",
    "    idf = presences.T.multiply(idf) \n",
    "    \n",
    "    print \"Computed tfNorm of shape: %d x %d (n° documents, n° queries terms)\" % tfNorm.shape\n",
    "    print \"Computed idf of shape: %d x %d (n° queries terms, n° queries)\" % idf.shape\n",
    "    \n",
    "    # Sparse representation\n",
    "    idf = sparse.csr_matrix(idf)\n",
    "    tfNorm = sparse.csr_matrix(tfNorm)\n",
    "    \n",
    "    # Too expensive to do as a matrix multiplication, thus do it sequentially\n",
    "    coicops = []\n",
    "    for idx in tqdm.tqdm(range(queries.shape[0])):\n",
    "        res = tfNorm.dot(idf[:,idx])\n",
    "        candidates = res != 0\n",
    "\n",
    "        indices = X_train.index[candidates.T.toarray()[0]]\n",
    "        scores = res[candidates.T.toarray()[0]].T.toarray()[0]\n",
    "\n",
    "        assert(len(indices) == len(scores))\n",
    "\n",
    "        if (len(indices) != 0):\n",
    "            knn = indices[np.argsort(-scores)[:5]]\n",
    "            # print knn\n",
    "            coicops.append(best(y_train.loc[knn]))\n",
    "        else:\n",
    "            coicops.append('no data')\n",
    "    return coicops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed tfNorm of shape: 298820 x 4563 (n° documents, n° queries terms)\n",
      "Computed idf of shape: 4563 x 74705 (n° queries terms, n° queries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74705/74705 [20:50<00:00, 59.75it/s]\n"
     ]
    }
   ],
   "source": [
    "coicops = score(X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      11111       0.93      0.93      0.93       217\n",
      "      11112       0.98      0.99      0.99      1241\n",
      "      11113       1.00      1.00      1.00         1\n",
      "      11114       0.70      0.78      0.74        18\n",
      "      11121       0.97      0.96      0.97       163\n",
      "      11122       0.98      0.98      0.98       879\n",
      "      11131       0.98      0.98      0.98       955\n",
      "      11132       0.95      0.92      0.94        90\n",
      "      11133       0.94      0.97      0.95       318\n",
      "      11134       0.89      1.00      0.94        58\n",
      "      11135       1.00      0.97      0.99       252\n",
      "      11136       0.96      0.94      0.95       953\n",
      "      11141       0.95      0.98      0.96       803\n",
      "      11142       0.95      0.97      0.96       183\n",
      "      11143       0.96      0.95      0.96      1197\n",
      "      11144       0.97      0.97      0.97      1808\n",
      "      11145       0.97      0.93      0.95       326\n",
      "      11151       0.95      0.95      0.95       175\n",
      "      11152       0.95      0.97      0.96       120\n",
      "      11153       0.91      0.93      0.92       419\n",
      "      11154       0.91      0.87      0.89       336\n",
      "      11155       0.93      0.96      0.95       247\n",
      "      11161       0.96      0.96      0.96      1808\n",
      "      11162       0.91      0.75      0.82       106\n",
      "      11163       0.97      0.97      0.97       106\n",
      "      11164       0.97      0.92      0.94       154\n",
      "      11165       0.98      0.96      0.97       163\n",
      "      11166       0.92      1.00      0.96       109\n",
      "      11171       0.98      0.96      0.97       199\n",
      "      11172       0.93      0.95      0.94       714\n",
      "      11173       0.99      0.98      0.99       707\n",
      "      11174       0.98      0.93      0.96       161\n",
      "      11175       0.95      0.95      0.95      1017\n",
      "      11181       0.96      0.97      0.97       295\n",
      "      11182       0.97      0.96      0.96       284\n",
      "      11183       0.99      0.99      0.99       176\n",
      "      11184       0.94      0.78      0.85        85\n",
      "      11211       1.00      0.50      0.67         6\n",
      "      11212       0.95      0.95      0.95        94\n",
      "      11213       0.96      0.94      0.95       166\n",
      "      11214       0.94      0.99      0.96       206\n",
      "      11215       0.99      1.00      0.99       458\n",
      "      11216       1.00      1.00      1.00         1\n",
      "      11221       0.92      0.96      0.94        89\n",
      "      11222       1.00      0.98      0.99        61\n",
      "      11223       0.98      0.96      0.97       151\n",
      "      11224       0.95      0.92      0.93        85\n",
      "      11231       0.99      0.96      0.98       161\n",
      "      11232       0.99      1.00      1.00       619\n",
      "      11241       1.00      1.00      1.00         2\n",
      "      11242       0.96      0.96      0.96        73\n",
      "      11243       0.93      0.99      0.96       100\n",
      "      11244       1.00      0.91      0.95        43\n",
      "      11251       0.99      0.99      0.99      1049\n",
      "      11252       0.99      0.96      0.97        93\n",
      "      11253       0.90      0.90      0.90        20\n",
      "      11261       1.00      1.00      1.00         1\n",
      "      11262       0.88      1.00      0.94        23\n",
      "      11263       0.50      1.00      0.67         2\n",
      "      11264       1.00      0.82      0.90        11\n",
      "      11265       0.94      0.73      0.82        22\n",
      "      11271       1.00      0.82      0.90        11\n",
      "      11281       0.98      0.99      0.99       575\n",
      "      11282       0.97      0.78      0.86        41\n",
      "      11283       0.96      0.96      0.96       308\n",
      "      11284       0.97      0.96      0.96       189\n",
      "      11291       0.98      0.97      0.98       104\n",
      "      11292       0.79      0.98      0.87        42\n",
      "      11311       0.95      0.94      0.95       157\n",
      "      11312       0.90      0.85      0.88        55\n",
      "      11321       1.00      0.99      0.99       151\n",
      "      11322       1.00      1.00      1.00        13\n",
      "      11323       0.90      0.90      0.90        39\n",
      "      11324       0.00      0.00      0.00         1\n",
      "      11331       0.85      0.86      0.86       114\n",
      "      11332       0.85      0.85      0.85        78\n",
      "      11341       0.95      0.89      0.92        47\n",
      "      11342       0.96      0.96      0.96       177\n",
      "      11351       0.96      0.98      0.97        54\n",
      "      11352       0.96      0.99      0.97       397\n",
      "      11361       0.96      0.93      0.95       692\n",
      "      11411       0.99      0.99      0.99       597\n",
      "      11412       0.75      1.00      0.86         3\n",
      "      11413       1.00      1.00      1.00         6\n",
      "      11421       0.98      0.99      0.99       411\n",
      "      11422       1.00      1.00      1.00      2454\n",
      "      11431       1.00      1.00      1.00        69\n",
      "      11432       1.00      1.00      1.00        21\n",
      "      11433       0.96      0.96      0.96        23\n",
      "      11434       1.00      0.75      0.86         4\n",
      "      11435       1.00      1.00      1.00         1\n",
      "      11441       0.99      0.99      0.99      1766\n",
      "      11442       0.98      0.99      0.98       211\n",
      "      11451       0.97      0.99      0.98       936\n",
      "      11452       0.96      0.93      0.95       143\n",
      "      11453       0.97      0.90      0.94       197\n",
      "      11454       1.00      1.00      1.00        69\n",
      "      11455       0.96      0.98      0.97       424\n",
      "      11456       0.94      0.93      0.94       258\n",
      "      11461       0.98      0.98      0.98       410\n",
      "      11462       0.93      0.92      0.93       105\n",
      "      11463       0.92      0.96      0.94       662\n",
      "      11464       0.94      0.96      0.95       199\n",
      "      11465       0.96      0.97      0.97       140\n",
      "      11471       1.00      1.00      1.00       965\n",
      "      11511       0.99      0.99      0.99       571\n",
      "      11521       0.85      0.90      0.88       122\n",
      "      11522       0.88      0.70      0.78        20\n",
      "      11523       0.91      0.89      0.90       237\n",
      "      11524       0.84      0.85      0.84       112\n",
      "      11531       0.99      0.95      0.97        99\n",
      "      11541       0.94      0.99      0.97       137\n",
      "      11551       0.96      0.96      0.96        25\n",
      "      11552       1.00      1.00      1.00        25\n",
      "      11611       1.00      0.97      0.98       274\n",
      "      11612       0.98      0.98      0.98       721\n",
      "      11621       1.00      1.00      1.00      1202\n",
      "      11631       0.99      0.99      0.99       776\n",
      "      11632       1.00      0.98      0.99       258\n",
      "      11633       0.96      0.94      0.95       640\n",
      "      11641       1.00      1.00      1.00       654\n",
      "      11642       0.99      0.98      0.99       786\n",
      "      11651       0.99      0.99      0.99       149\n",
      "      11652       0.97      0.95      0.96       419\n",
      "      11661       0.90      0.86      0.88        21\n",
      "      11671       0.97      0.95      0.96       283\n",
      "      11672       0.96      0.97      0.97       605\n",
      "      11673       0.96      1.00      0.98        52\n",
      "      11681       0.98      0.99      0.98       151\n",
      "      11682       0.82      0.89      0.85       201\n",
      "      11711       0.99      0.99      0.99       480\n",
      "      11712       0.96      0.95      0.96       147\n",
      "      11713       0.95      0.97      0.96       328\n",
      "      11714       0.97      0.98      0.98       560\n",
      "      11721       0.99      0.99      0.99       315\n",
      "      11722       1.00      0.99      1.00       105\n",
      "      11723       0.99      0.98      0.99       587\n",
      "      11731       1.00      1.00      1.00       576\n",
      "      11732       0.99      0.99      0.99       768\n",
      "      11733       0.99      1.00      0.99        95\n",
      "      11734       0.99      1.00      1.00       258\n",
      "      11735       1.00      1.00      1.00      1099\n",
      "      11741       0.99      0.99      0.99       666\n",
      "      11742       0.97      0.99      0.98       313\n",
      "      11744       0.99      0.98      0.99       250\n",
      "      11751       1.00      0.99      0.99       690\n",
      "      11752       0.99      1.00      0.99        98\n",
      "      11753       1.00      0.99      1.00       938\n",
      "      11754       0.98      0.99      0.98       427\n",
      "      11755       0.99      0.99      0.99       541\n",
      "      11756       0.95      0.95      0.95       295\n",
      "      11761       0.97      0.98      0.97       155\n",
      "      11762       1.00      1.00      1.00        11\n",
      "      11763       0.96      0.89      0.92       218\n",
      "      11771       0.86      0.60      0.71        10\n",
      "      11772       0.91      0.80      0.85        60\n",
      "      11781       0.98      0.99      0.98       426\n",
      "      11782       0.99      0.99      0.99       213\n",
      "      11783       1.00      1.00      1.00       484\n",
      "      11784       0.95      0.98      0.96       197\n",
      "      11785       1.00      0.78      0.88        18\n",
      "      11786       0.99      0.93      0.96       228\n",
      "      11787       1.00      0.95      0.97        78\n",
      "      11791       1.00      0.75      0.86         8\n",
      "      11792       0.97      0.99      0.98       467\n",
      "      11793       0.97      0.91      0.94       396\n",
      "      11811       1.00      1.00      1.00       377\n",
      "      11821       0.98      0.99      0.98       156\n",
      "      11822       0.99      0.99      0.99        77\n",
      "      11823       0.97      0.98      0.98       166\n",
      "      11824       0.97      0.99      0.98        96\n",
      "      11831       0.92      0.88      0.90        41\n",
      "      11832       0.93      0.85      0.89       139\n",
      "      11841       0.96      0.97      0.96      1223\n",
      "      11842       0.95      0.94      0.94      1269\n",
      "      11851       1.00      0.99      1.00       164\n",
      "      11852       0.91      0.99      0.95        76\n",
      "      11853       0.96      0.98      0.97      1002\n",
      "      11854       0.80      0.92      0.86        77\n",
      "      11861       0.99      0.99      0.99      1538\n",
      "      11862       0.94      0.95      0.94       645\n",
      "      11871       0.96      0.98      0.97       249\n",
      "      11872       0.97      0.94      0.95       321\n",
      "      11873       0.93      0.91      0.92       133\n",
      "      11911       0.99      0.93      0.96       147\n",
      "      11912       0.96      0.96      0.96      1516\n",
      "      11913       0.98      0.99      0.98       476\n",
      "      11921       0.97      0.99      0.98        70\n",
      "      11923       0.98      0.97      0.98       309\n",
      "      11931       0.96      0.98      0.97       278\n",
      "      11932       0.77      0.69      0.73        39\n",
      "      11933       1.00      0.60      0.75        30\n",
      "      11941       0.96      0.95      0.96       128\n",
      "      11951       0.99      0.99      0.99       697\n",
      "      11952       0.98      0.98      0.98       158\n",
      "      11962       0.99      0.99      0.99       176\n",
      "      12111       0.99      0.96      0.97       146\n",
      "      12112       0.98      0.98      0.98       377\n",
      "      12113       0.00      0.00      0.00         1\n",
      "      12121       0.99      0.99      0.99       370\n",
      "      12131       0.95      0.96      0.96       108\n",
      "      12132       0.92      0.96      0.94        25\n",
      "      12211       0.99      0.99      0.99       500\n",
      "      12221       0.97      0.97      0.97      1554\n",
      "      12222       0.97      0.97      0.97       995\n",
      "      12231       0.97      0.94      0.95       202\n",
      "      12232       0.96      0.98      0.97       269\n",
      "      12241       0.99      0.99      0.99      1042\n",
      "      12251       1.00      0.81      0.89        21\n",
      "      14111       0.95      1.00      0.97       225\n",
      "      14112       0.90      0.90      0.90        62\n",
      "      14113       1.00      0.67      0.80        27\n",
      "      14211       0.99      0.99      0.99      1126\n",
      "      14212       0.99      0.94      0.96        94\n",
      "      14213       0.96      0.95      0.95        55\n",
      "      14214       0.99      0.99      0.99       204\n",
      "      14215       0.89      0.78      0.83        32\n",
      "      14311       0.98      0.98      0.98       317\n",
      "      14312       0.99      0.97      0.98       437\n",
      "    no data       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.97      0.97      0.97     74705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 digits coicops metrics\n",
    "print(metrics.classification_report(y_test, coicops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       1111       0.98      0.99      0.99      1477\n",
      "       1112       0.98      0.98      0.98      1042\n",
      "       1113       0.98      0.97      0.98      2626\n",
      "       1114       0.98      0.98      0.98      4317\n",
      "       1115       0.98      0.98      0.98      1297\n",
      "       1116       0.97      0.96      0.97      2446\n",
      "       1117       0.96      0.97      0.97      2798\n",
      "       1118       0.98      0.96      0.97       840\n",
      "       1121       0.98      0.99      0.99       931\n",
      "       1122       0.98      0.97      0.98       386\n",
      "       1123       1.00      0.99      1.00       780\n",
      "       1124       0.97      0.98      0.97       218\n",
      "       1125       0.99      0.99      0.99      1162\n",
      "       1126       0.96      0.93      0.95        59\n",
      "       1127       1.00      0.82      0.90        11\n",
      "       1128       0.98      0.98      0.98      1113\n",
      "       1129       0.92      0.97      0.94       146\n",
      "       1131       0.95      0.92      0.94       212\n",
      "       1132       0.98      0.98      0.98       204\n",
      "       1133       0.94      0.94      0.94       192\n",
      "       1134       0.95      0.95      0.95       224\n",
      "       1135       0.96      0.99      0.97       451\n",
      "       1136       0.96      0.93      0.95       692\n",
      "       1141       0.99      0.99      0.99       606\n",
      "       1142       1.00      1.00      1.00      2865\n",
      "       1143       1.00      0.99      1.00       118\n",
      "       1144       0.99      0.99      0.99      1977\n",
      "       1145       0.98      0.99      0.99      2027\n",
      "       1146       0.95      0.97      0.96      1516\n",
      "       1147       1.00      1.00      1.00       965\n",
      "       1151       0.99      0.99      0.99       571\n",
      "       1152       0.97      0.97      0.97       491\n",
      "       1153       0.99      0.95      0.97        99\n",
      "       1154       0.94      0.99      0.97       137\n",
      "       1155       0.98      0.98      0.98        50\n",
      "       1161       0.98      0.98      0.98       995\n",
      "       1162       1.00      1.00      1.00      1202\n",
      "       1163       0.99      0.97      0.98      1674\n",
      "       1164       1.00      0.99      0.99      1440\n",
      "       1165       0.98      0.96      0.97       568\n",
      "       1166       0.90      0.86      0.88        21\n",
      "       1167       0.97      0.97      0.97       940\n",
      "       1168       0.89      0.93      0.91       352\n",
      "       1171       0.98      0.98      0.98      1515\n",
      "       1172       0.99      0.99      0.99      1007\n",
      "       1173       0.99      0.99      0.99      2796\n",
      "       1174       0.99      0.99      0.99      1229\n",
      "       1175       0.99      0.99      0.99      2989\n",
      "       1176       0.97      0.93      0.95       384\n",
      "       1177       0.90      0.77      0.83        70\n",
      "       1178       0.99      0.98      0.99      1644\n",
      "       1179       0.98      0.96      0.97       871\n",
      "       1181       1.00      1.00      1.00       377\n",
      "       1182       0.98      0.99      0.98       495\n",
      "       1183       0.93      0.86      0.89       180\n",
      "       1184       0.97      0.97      0.97      2492\n",
      "       1185       0.96      0.98      0.97      1319\n",
      "       1186       0.98      0.98      0.98      2183\n",
      "       1187       0.98      0.97      0.98       703\n",
      "       1191       0.97      0.97      0.97      2139\n",
      "       1192       0.98      0.97      0.98       379\n",
      "       1193       0.99      0.96      0.97       347\n",
      "       1194       0.96      0.95      0.96       128\n",
      "       1195       0.99      0.99      0.99       855\n",
      "       1196       0.99      0.99      0.99       176\n",
      "       1211       0.99      0.98      0.99       524\n",
      "       1212       0.99      0.99      0.99       370\n",
      "       1213       0.96      0.98      0.97       133\n",
      "       1221       0.99      0.99      0.99       500\n",
      "       1222       0.99      0.99      0.99      2549\n",
      "       1223       0.97      0.97      0.97       471\n",
      "       1224       0.99      0.99      0.99      1042\n",
      "       1225       1.00      0.81      0.89        21\n",
      "       1411       0.96      0.96      0.96       314\n",
      "       1421       0.99      0.99      0.99      1511\n",
      "       1431       0.99      0.98      0.99       754\n",
      "       no d       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.98      0.98      0.98     74705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 digits coicops metrics\n",
    "print(metrics.classification_report([x[:4] for x in y_test], \n",
    "                                    [x[:4] for x in coicops]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
